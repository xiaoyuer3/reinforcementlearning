强化学习-实现通用人工智能的重要途经之一
强化学习（维基百科定义）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。
(通俗的定义)强化学习（Reinforcement learning，RL）讨论的问题是一个智能体(agent)怎么在一个复杂不确定的 环境(environment)里面去极大化它能获得的奖励。通过感知所处环境的 状态(state)对 动作(action)的 反应(reward)， 来指导更好的动作，从而获得最大的 收益(return)这被称为在交互中学习，这样的学习方法就被称作强化学习。

1. 从AlphaGo到AlphaZero:强化学习的进化之路

- AlphaGo:学习人类棋谱阶段
  2015年10月，AlphaGo以5∶0的比分战胜了欧洲围棋冠军樊麾（Fan Hui），这是围棋历史上机器第1次战胜职业围棋选手
  2016年1月26日，谷歌旗下的DeepMind团队在《Nature》杂志发表《Mastering the game of Go with deep neural networks and tree search》，从而揭开了围棋人机大战的历史性一页
  2016年3月，AlphaGo Lee以4∶1的比分战胜了围棋九段高手李世石。这也是对于强化学习推广影响最大的一次事件。
  2017年5月，在中国乌镇举行的人工智能峰会上，排名世界第一的围棋冠军柯洁挑战AlphaGo Master，最终以0∶3落败。
- AlphaGo Zero:无需棋谱，仅通过规则学习
  突破人类已有的先验知识，能够探索到全新的下棋方式
  2017年10月，DeepMind团队公布进化最强版AlphaGo Zero，这个版本最大的特征是不再需要人类经验数据，用于训练的是机器自我对弈所产生的数据。在经过3天的训练后，AlphaGo Zero就可以战胜AlphaGo Lee，比分高达100∶0；而经过40天的训练后，就以89∶11的比分击败了AlphaGo Master
  2017年10月19日，DeepMind公司在《Nature》杂志发布了一篇新的论文，AlphaGo Zero——AlphaGo Zero击败了Master版本。特点：它完全不依赖人类棋手的经验
- AlphaZero:通用的棋类学习算法
  用于围棋，国际象棋，日本将棋
  2018年12月，《Science》杂志发表论文《A general reinforcement learning algorithm that masters chess,shogi and Go through self-play》（《用通用强化学习算法自我对弈,掌握国际象棋、将棋和围棋》）。论文揭示，DeepMind依据之前的经验，采用新算法开发了单一系统AlphaZero，这套系统竟然在短期的自我学习中，成功地实现了对国际象棋、日本将棋及围棋目前最强智能系统的完胜

象棋vs围棋的下棋难度对比:如何走向真正的智能
国际象棋的搜索宽度大概是30，搜索深度大概是80，整个搜索空间大约为10的50次方，1997年IBM“深蓝”战胜了卡斯帕罗夫就是通过暴力枚举的方式，成为历史上第1个战胜人类国际象棋大师的下棋机
而围棋的搜索宽度大概为250，搜索深度大概是150，搜索空间在10的170次方以上，比宇宙中的粒子数10的80次方还多。所以通过暴力枚举的方式即使通过计算机也是不可能实现的


Alpha Zero VS Alpha Go
- 无需先验知识。不再需要人为手工设计特征，而是仅利用棋盘上的黑白棋子的摆放情况，作为原始输入数据，将其输入到神经网络中，以此得到结果。
- 神经网络结构复杂性降低。原先两个结构独立的策略网络和价值网络合为一体，合并成一个神经网络
- 舍弃快速走子网络。不再使用快速走子网络进行随机模拟，而是完全将神经网络得到的结果替换随机模拟，从而在提升学习速率的同时，增强了神经网络估值的准确性
- 神经网络引入残差结构。神经网络采用基于残差网络结构的模块进行搭建，用了更深的神经网络进行特征表征提取。从而能在更加复杂的棋盘局面中进行学习。

Alpha Zero原理
核心算法：蒙特卡洛树搜索+深度学习算法
paper:《Mastering the Game of Go without Human Knowledge》
具体拆分成三个部分：
策略网络：输出每个落子的概率
估值网络：评估当前棋面的局势
蒙特卡洛树搜索：通过不断的实验抽样来训练模型


蒙特卡洛树搜索
MCTS算法生成的对弈可以作为神经网络的训练数据
1987年Bruce Abramson在他的博士论文中提出了基于蒙特卡洛方法的树搜索这一想法。这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率.

神经网络（CNN网络）

2. 强化学习与有监督学习和无监督学习的对比
   强化学习区别于有监督学习和无监督学习，是基于“试错”机制的一种新的机器学习方式

有监督学习：从外部监督者提供的带标注训练集中进行学习。特点：任务驱动型
无监督学习：是一个典型的寻找未标注数据中隐含结构的过程。特点：数据驱动型
强化学习：更偏重于智能体与环境的交互， 这带来了一个独有的挑战 ——“试错（exploration）”与“
开发（exploitation）”之间的折中权衡，智能体必须开发已有的经验来获取收益，同时也要进行试探，使得未来可以获得更好的动作选择空间。 特点：从试错中学习
典型特点：
- 试错学习：强化学习一般没有直接的指导信息，Agent 要以不断与 Environment 进行交互，通过试错的方式来获得最佳策略(Policy)。
- 延迟回报：强化学习的指导信息很少，而且往往是在事后（最后一个状态(State)）才给出的。比如 围棋中只有到了最后才能知道胜负。
3. 强化学习的发展历史
- 明斯基（Marvin Minsky:被誉为人工智能之父）首次提出强化学习概念。
    1. 1954年，他的博士论文《Theory of neural-analog reinforcement systems and its application to the brain-model problem》发表，其核心内容就是提出了一种解决“brain-model”的新方法（也就是模仿人脑的工作方式来设计计算机算法问题）。在这篇论文中，明斯基提到了“reinforcement operator”、“reinforcement process”、“reinforcement system”等概念以及试错学习等
    2. 1961年发表的论文《Steps Toward Artificial Intelligence》，影响较大，被认为正式提出强化学习的起点
- 发展阶段：独立的对两个领域的研究
    - 心理学中的动物学习：早期人工智能算法研究的方向之一。早期人工智能的部分研究方向就是努力模仿人类大脑和动物学习的过程来建立模型和算法。
        - 强化学习重要的两种概念的来源：试错学习和时间差分学习(TD learning)
    - 最优控制：最优控制是另一种与强化学习密切相关的研究领域。在上个世纪50年代，最优控制用来描述设计控制器以最小化动态系统随时间推移行为的衡量标准的问题。
      出现的重要理论：
        - 贝尔曼方程：使用动力系统状态和值函数或”最佳返回函数”的概念来定义一个函数方程

bellman方程的基本形态，当前状态的价值和下一步的价值以及当前反馈reward有关，它表明了value function可以通过迭代来进行计算
- 动态规划：求解最优控制问题的一类方法被称为动态规划
- 马尔可夫数学建模：MDP是这里最优控制问题的一种描述或者说数学建模
- 1989年，Chris Watkins提出了Q Learning的模型，它是时间差分与最优控制的结合。也将之前不同强化学习发展的路径统一到了一起。学习到的知识通过一个称为状态－动作值函数的表结构来存储。通常用符号Q(s,a)来表示这个函数，这就是所谓Q-learning的来历。
  贡献：统一了强化学习的基础理论，解决了很多基础理论问题
  缺陷：仅限于状态空间和动作空间有限的学习任务
- 2013年，DeepMind提出了基于深度学习的强化学习模型，即Deep Q Network，并在2015年发表了改进版本。它采用深度神经网络来逼近值函数来解决连续状态动作空间问题，代表算法DQN
4. 强化学习的基本要素

- 环境：是一个外部系统，智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动
- 智能体：是一个嵌入到环境中的系统，能够通过采取行动来改变环境的状态
- 状态/观测：状态是对世界的完整描述，不会隐藏世界的信息。观测是对状态的部分描述，可能会遗漏一些信息
- 行为：不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间，包括离散动作空间和连续动作空间
- 奖励：是由环境给的一个标量的反馈信号，这个信号显示了智能体在某一步采取了某个策略的表现
  强化学习的训练学习过程：
  在强化学习过程中，智能体跟环境一直在交互。智能体在环境里面获取到状态，智能体会利用这个状态输出一个动作，一个决策。然后这个决策会放到环境之中去，环境会根据智能体采取的决策，输出下一个状态以及当前的这个决策得到的奖励。智能体的目的就是为了尽可能多地从环境中获取奖励
  Flappy bird游戏：https://enhuiz.github.io/flappybird-ql/

5. 强化学习的算法分类：从Q-Learning到DQN

- Q-Learning：适用于状态空间和动作空间有限的情况下
  Q-Learning是一种off-policy TD方法

使用Q-table来存储Q值

Q值的更新公式：使用贝尔曼方程加贪心策略得到target-Q,不断逼近当前策略预测Q值

- DQN：使用值函数近似 DQN=Q-Learning+DNN
  当状态和动作空间是离散且维数不高时可使用Q-Table储存每个状态动作对的Q值，而当状态和动作空间是高维连续时，需要使用值函数近似

关键问题：
1. 如何定义Loss函数？估计网络和target网络？MC or TD
2. 如何产生足够多的训练样本？通过Q-Learning使用reward来构造标签
3. 样本独立同分布？通过experience replay（经验池）的方法来解决相关性及非静态分布问题
   训练步骤：

损失函数设计：

DQN的变形：
- Double-DQN：将动作选择和价值估计分开，避免价值过高估计
- Dueling-DQN：将Q值分解为状态价值和优势函数，得到更多有用信息
- Prioritized Replay Buffer：将经验池中的经验按照优先级进行采样
  参考资料：https://zhuanlan.zhihu.com/p/35882937?utm_medium=social&utm_oi=706756979481149440&utm_psn=1579186736969781248&utm_source=wechat_session
6. 强化学习在搜广推的应用

- 阿里电商场景下搜广推应用强化学习，算法指标提升10%-20%
    - 阿里强化学习白皮书： 强化学习在阿里的技术演进与业务创新
- YouTube 成功将 RL 应用在了推荐场景，并且取得了 YouTube 近几年来最显著的线上收益。
  发布两篇工业界强化学习parper
    - Top-K Off-Policy Correction for a REINFORCE Recommender System
    -  Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology
- 京东在强化学习的探索：
    - 强化学习在京东618大促流量调控中的落地应用
    - 强化学习在京东广告序列推荐中的应用
- 美团强化学习实践：强化学习在美团“猜你喜欢”的实践
7. 挑战
- 仿真环境设计
- 奖励机制设计
- 控制与最佳策略选择
- 探索利用问题
- 数据利用效率
- 不稳定与不收敛问题
  参考资料
- David Silver强化学习公开课笔记
- 李宏毅强化学习笔记
- 知乎大佬的笔记：智能单元

